{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Paper: _Lin, T. Y., Goyal, P., Girshick, R., He, K., & DollÃ¡r, P. (2017). Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision (pp. 2980-2988)_.\n",
    "\n",
    "- Focal loss is used to give a higher loss values to the examples on which the model performed worst. Ex: if the GT is 1.0 and the model predicted 0.2 --> bad pred, so focal loss is high; if the GT is 1.0 and the model predicted 0.8 --> good prediction, so focal loss is low. But this is pretty generic, in fact it's true for all losses, like the crossentropy.\n",
    "\n",
    "\n",
    "- The key is that the focal loss is a modified crossentropy. In fact, the focal loss is composed by two factor, let's say `A * B`, where `B` is the crossentropy while `A` is a multiplier of the crossentropy, a multiplier with range [0, 1], so actually the focal loss is a reduce crossentropy. The point is that for examples correctly classified by the model (ex: gt=1 and pred=0.9, or gt=0 and pred=0.1), the focal loss is a greatly reduced crossentropy (ce=0.1, fl=0.001, with fl's gamma=2). This is the power of focal loss: reducing the contribution of the well classified examples (usually examples found frequently in the dataset) to the total loss (like the total batch loss), so that the bad classified examples (usually the rarest examples) can stand out more, so that the training process can focus on them. It's is just a matter of proportion and not of \"true value\", in fact for badly classified examples (ex: gt=1 and pred=0.1, or gt=0 and pred=0.9), the focal loss is actually lower than the crossentropy (fl=1.87, ce=2.30).\n",
    "\n",
    "- Focal loss formula (for a binary classification task) (y = gt, p = pred):\n",
    "    - two cases:\n",
    "        - if `y = 1` --> `-(1-p)**gamma * log(p)`\n",
    "        - if `y = 0` --> `-(p)**gamma * log(1-p)`\n",
    "    - the second factor is exactly the binary crossentropy, for the cases y=1 and y=0.\n",
    "    - so, when gamma = 0, the focal loss is equal to the crossentropy.\n",
    "    - when can also add another factor, an \"alpha\" parameter representing weights we can give to the different classes, to down-weight even more the contribution of the frequent classes and up-weight the impact of the rare classes (actually, since these weights are usually in the range [0, 1], technically we are just downweighting everything, just some classes more than others). This weighting factor alpha can be added also to the standard crossentropy, it's not peculiar of the focal loss.\n",
    "    <p>\n",
    "\n",
    "- For multiclass task there are two ways to compute the focal loss for each example: (1) considering only the prediction of the true class, and (2) considering the predictions for all the classes, even the wrong ones. In the second case we are saying \"the correct class is A, and the model should have said neither class B or C, but let's still consider that it predicted 0.1 for class B and 0.2 for class B, along with the prediction 0.7 for class A\". Instead, in the first case we only consider the prediction of the correct class A 0.7, thus implying that there's only one wrong class and its prediction is 0.3 (basically we are falling back to the binary case). The first approach has been said to be \"the most common\", while the second one is more thorough. Here we use the first method, but at the end classes for both are provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <ins> THEORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_bce_loss(pred, gt):\n",
    "    ce = - (gt * np.log(pred) + (1-gt) * np.log(1-pred))\n",
    "    return ce\n",
    "\n",
    "def simple_focal_loss(pred, gt, gamma=2, alpha=1):\n",
    "    pt = pred if gt == 1 else 1 - pred\n",
    "    fl = - alpha * ((1 - pt)**gamma) * np.log(pt)\n",
    "    return fl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10536051565782628\n",
      "0.10536051565782628\n",
      "2.3025850929940455\n",
      "2.302585092994046\n"
     ]
    }
   ],
   "source": [
    "print(simple_bce_loss(pred=0.9, gt=1))\n",
    "print(simple_bce_loss(pred=0.1, gt=0))\n",
    "print(simple_bce_loss(pred=0.1, gt=1))\n",
    "print(simple_bce_loss(pred=0.9, gt=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0010536051565782623\n",
      "0.0010536051565782623\n",
      "1.865093925325177\n",
      "1.8650939253251773\n"
     ]
    }
   ],
   "source": [
    "print(simple_focal_loss(pred=0.9, gt=1, gamma=2))\n",
    "print(simple_focal_loss(pred=0.1, gt=0, gamma=2))\n",
    "print(simple_focal_loss(pred=0.1, gt=1, gamma=2))\n",
    "print(simple_focal_loss(pred=0.9, gt=0, gamma=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demostration of how the focal loss makes the badly classified examples stand out more.\n",
    "# Let's say we have a batch of two example, one of which is well classified while the other not\n",
    "gt1 = 1\n",
    "pred1 = 0.9\n",
    "gt2 = 1\n",
    "pred2 = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.10536051565782628, 2.3025850929940455)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The batch loss is the sum of their losses\n",
    "ce_loss_1 = simple_bce_loss(pred=pred1, gt=gt1)\n",
    "ce_loss_2 = simple_bce_loss(pred=pred2, gt=gt2)\n",
    "ce_batch_loss = ce_loss_1 + ce_loss_2\n",
    "\n",
    "ce_loss_1, ce_loss_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.04375535530340077, 0.9562446446965992)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see the proportional impact of each example of the batch loss\n",
    "ce_loss_1 / ce_batch_loss, ce_loss_2 / ce_batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0010536051565782623, 1.865093925325177)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's do the same with the focal loss\n",
    "fl_loss_1 = simple_focal_loss(pred=pred1, gt=gt1, gamma=2)\n",
    "fl_loss_2 = simple_focal_loss(pred=pred2, gt=gt2, gamma=2)\n",
    "fl_batch_loss = fl_loss_1 + fl_loss_2\n",
    "\n",
    "fl_loss_1, fl_loss_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0005645883507968253, 0.9994354116492032)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can see that the contribution of the well classified example to the batch loss is way lower\n",
    "fl_loss_1 / fl_batch_loss, fl_loss_2 / fl_batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7.74906520057872e-07, 0.9999992250934798)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And higher the gamma, even lower the contribution of the bad classified examples\n",
    "fl_loss_1 = simple_focal_loss(pred=pred1, gt=gt1, gamma=5)\n",
    "fl_loss_2 = simple_focal_loss(pred=pred2, gt=gt2, gamma=5)\n",
    "fl_batch_loss = fl_loss_1 + fl_loss_2\n",
    "fl_loss_1 / fl_batch_loss, fl_loss_2 / fl_batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35667494393873245\n",
      "0.35667494393873245\n"
     ]
    }
   ],
   "source": [
    "# When gamma = 0, focal loss is the crossentropy loss.\n",
    "print(simple_focal_loss(pred=0.7, gt=1, gamma=0))\n",
    "print(simple_bce_loss(pred=0.7, gt=1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <ins> INNER WORKING"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FOR BINARY CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.tensor([1., 0., 0., 1., 1., 0., 1., 0.])\n",
    "pred = torch.tensor([0.9, 0.3, 0.2, 0.3, 0.6, 0.4, 0.1, 0.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.]])\n",
      "tensor([[0.1000, 0.9000],\n",
      "        [0.7000, 0.3000],\n",
      "        [0.8000, 0.2000],\n",
      "        [0.7000, 0.3000],\n",
      "        [0.4000, 0.6000],\n",
      "        [0.6000, 0.4000],\n",
      "        [0.9000, 0.1000],\n",
      "        [0.2000, 0.8000]])\n"
     ]
    }
   ],
   "source": [
    "# To matrix form\n",
    "target = torch.stack((1 - target, target), dim=1)\n",
    "pred = torch.stack((1 - pred, pred), dim=1)\n",
    "\n",
    "print(target)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1000, 0.0000],\n",
       "        [0.0000, 0.3000],\n",
       "        [0.0000, 0.2000],\n",
       "        [0.7000, 0.0000],\n",
       "        [0.4000, 0.0000],\n",
       "        [0.0000, 0.4000],\n",
       "        [0.9000, 0.0000],\n",
       "        [0.0000, 0.8000]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We compute the two focal loss factors separately.\n",
    "# Multiplication by `1-target` to ensure that when y=1, we use `1-p` and \n",
    "# when y=0, we use `p`, as by the formula.\n",
    "a = (1 - target) * pred\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0000, -0.1054],\n",
       "        [-0.3567, -0.0000],\n",
       "        [-0.2231, -0.0000],\n",
       "        [-0.0000, -1.2040],\n",
       "        [-0.0000, -0.5108],\n",
       "        [-0.5108, -0.0000],\n",
       "        [-0.0000, -2.3026],\n",
       "        [-1.6094, -0.0000]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we use `target` in order to use `p` if y=1 and `1-p` if y=0 (opposite of above)\n",
    "b = target * torch.log(pred)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1000, 0.3000, 0.2000, 0.7000, 0.4000, 0.4000, 0.9000, 0.8000])\n",
      "tensor([-0.1054, -0.3567, -0.2231, -1.2040, -0.5108, -0.5108, -2.3026, -1.6094])\n"
     ]
    }
   ],
   "source": [
    "# Flatten to 1 dimension, otherwise if matrix multiplication of between matrices yields a zero matrix\n",
    "c = torch.sum(a, dim=1)\n",
    "d = torch.sum(b, dim=1)\n",
    "\n",
    "print(c)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0300, 0.0100, 0.5900, 0.0800, 0.0800, 1.8700, 1.0300])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Focal loss computation for each example\n",
    "gamma = 2\n",
    "fl = - torch.pow(c, gamma) * d\n",
    "fl = torch.round(fl, decimals=2)\n",
    "fl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9000, 0.9000],\n",
       "        [0.7000, 0.7000],\n",
       "        [0.8000, 0.8000],\n",
       "        [0.3000, 0.3000],\n",
       "        [0.6000, 0.6000],\n",
       "        [0.6000, 0.6000],\n",
       "        [0.1000, 0.1000],\n",
       "        [0.2000, 0.2000]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another way to use `p` or `1-p` in function of y, is by using `torch.where`,\n",
    "# which corresponds to the use of the variable `pt` in the paper.\n",
    "pt = torch.where(target == 1, pred, 1 - pred)\n",
    "pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0300, 0.0100, 0.5900, 0.0800, 0.0800, 1.8700, 1.0300])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep just one column since they are the same\n",
    "pt = pt[..., 0]\n",
    "fl = - torch.pow((1 - pt), gamma) * torch.log(pt)\n",
    "fl = torch.round(fl, decimals=2)\n",
    "fl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FOR MULTICLASS CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.tensor([\n",
    "    [1, 0, 0],\n",
    "    [0, 0, 1],\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 0],\n",
    "    [0, 1, 0],\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = torch.tensor([\n",
    "    [0.6828, 0.2600, 0.0572],\n",
    "    [0.4128, 0.0719, 0.5153],\n",
    "    [0.7387, 0.1748, 0.0865],\n",
    "    [0.6274, 0.1481, 0.2245],\n",
    "    [0.2029, 0.5384, 0.2587],\n",
    "    [0.5034, 0.0887, 0.4079],\n",
    "    [0.4194, 0.2748, 0.3058],\n",
    "    [0.4968, 0.1833, 0.3198]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As explained at the beginning of the notebook, when computing the focal loss \n",
    "# for multi-class classification problems, it's common to calculate the loss \n",
    "# for the true class only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6828, 0.7400, 0.9428],\n",
       "        [0.5872, 0.9281, 0.5153],\n",
       "        [0.7387, 0.8252, 0.9135],\n",
       "        [0.6274, 0.8519, 0.7755],\n",
       "        [0.7971, 0.5384, 0.7413],\n",
       "        [0.5034, 0.9113, 0.5921],\n",
       "        [0.4194, 0.7252, 0.6942],\n",
       "        [0.4968, 0.8167, 0.6802]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Following the approach above...\n",
    "pt = torch.where(target == 1, pred, 1 - pred)\n",
    "pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0384, -0.0000, -0.0000],\n",
       "        [-0.0000, -0.0000, 0.1558],\n",
       "        [0.0207, -0.0000, -0.0000],\n",
       "        [0.0647, -0.0000, -0.0000],\n",
       "        [-0.0000, 0.1319, -0.0000],\n",
       "        [0.1693, -0.0000, -0.0000],\n",
       "        [0.2929, -0.0000, -0.0000],\n",
       "        [0.1771, -0.0000, -0.0000]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiply by the target to keep only the `p` of the true class\n",
    "fl = - target * torch.pow(1 - pt, gamma) * torch.log(pt)\n",
    "fl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0384, 0.1558, 0.0207, 0.0647, 0.1319, 0.1693, 0.2929, 0.1771])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fl = torch.sum(fl, dim=1)\n",
    "\n",
    "fl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6828, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.5153],\n",
       "        [0.7387, 0.0000, 0.0000],\n",
       "        [0.6274, 0.0000, 0.0000],\n",
       "        [0.0000, 0.5384, 0.0000],\n",
       "        [0.5034, 0.0000, 0.0000],\n",
       "        [0.4194, 0.0000, 0.0000],\n",
       "        [0.4968, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On another perspective, since as said before for a multiclass task it's common \n",
    "# to calculate the loss for the true class only, basically we are falling back\n",
    "# to the binary case.\n",
    "\n",
    "# We start by keeping only the `p` for the real class.\n",
    "a = target * pred\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6828, 0.5153, 0.7387, 0.6274, 0.5384, 0.5034, 0.4194, 0.4968])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Then we put everything in one column...\n",
    "b = torch.sum(a, dim=1)\n",
    "\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3172, 0.6828],\n",
      "        [0.4847, 0.5153],\n",
      "        [0.2613, 0.7387],\n",
      "        [0.3726, 0.6274],\n",
      "        [0.4616, 0.5384],\n",
      "        [0.4966, 0.5034],\n",
      "        [0.5806, 0.4194],\n",
      "        [0.5032, 0.4968]])\n",
      "tensor([[0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# ...which is equivalent to these pred and target matrices\n",
    "pred2 = torch.stack([1 - b, b], dim=1)\n",
    "\n",
    "target2 = torch.stack([ # the target reflects the position of the true class `p`\n",
    "    torch.zeros(size=[8,]),\n",
    "    torch.ones(size=[8,])\n",
    "    ],\n",
    "    dim = 1\n",
    ")\n",
    "\n",
    "print(pred2)\n",
    "print(target2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0384, 0.1558, 0.0207, 0.0647, 0.1319, 0.1693, 0.2929, 0.1771])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we don't need any summing to bring all `p` to the same column,\n",
    "# since it was already built that way, so we can easily compute fl\n",
    "fl = - torch.pow(1 - b, gamma) * torch.log(b)\n",
    "\n",
    "fl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOR MULTICLASS SEGMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - understand how compute fl for images (use flattening first)\n",
    "# - after \"inner working\" chapter, add chapter (title 1) with the proper class functions (with alpha)\n",
    "\n",
    "# [B, C, H, W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = torch.tensor([\n",
    "    [2, 2, 2, 0, 0, 0, 0, 0],\n",
    "    [2, 2, 0, 3, 3, 3, 3, 0],\n",
    "    [2, 0, 0, 3, 3, 3, 3, 0],\n",
    "    [0, 0, 0, 3, 3, 3, 3, 0],\n",
    "    [0, 0, 0, 3, 3, 3, 3, 0],\n",
    "    [0, 0, 0, 3, 3, 3, 3, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [1, 1, 0, 0, 0, 0, 0, 0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3])\n",
      "tensor([0.5625, 0.0312, 0.0938, 0.3125])\n"
     ]
    }
   ],
   "source": [
    "# Class frequency\n",
    "classes, counts = torch.unique(gt, return_counts=True)\n",
    "\n",
    "print(classes)\n",
    "print(counts / (gt.shape[0] * gt.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 8, 8])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To one-hot\n",
    "target = torch.nn.functional.one_hot(gt, num_classes=4).permute(2, 0, 1).unsqueeze(0) # [B, C, H, W]\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 8, 8])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate random prediction (high loss expected)\n",
    "torch.manual_seed(2296)\n",
    "pred_bad = torch.randn(size=(1, 4, 8, 8))\n",
    "pred_bad = torch.nn.Softmax(dim=1)(pred_bad)\n",
    "pred_bad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 8, 8])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a prediction close to the gt (low loss expected)\n",
    "pred_labels = torch.tensor([\n",
    "    [2, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [2, 2, 0, 3, 2, 2, 2, 0],\n",
    "    [2, 0, 0, 3, 3, 2, 1, 0],\n",
    "    [0, 0, 0, 3, 3, 3, 1, 0],\n",
    "    [1, 1, 0, 3, 3, 3, 3, 0],\n",
    "    [0, 0, 0, 3, 3, 3, 3, 0],\n",
    "    [0, 0, 0, 0, 0, 1, 0, 0],\n",
    "    [1, 2, 0, 0, 0, 1, 0, 0]\n",
    "])\n",
    "\n",
    "pred_good = torch.nn.functional.one_hot(pred_labels, num_classes=4).permute(2, 0, 1).unsqueeze(0).type(torch.float32)\n",
    "pred_good = torch.nn.Softmax(dim=1)(pred_good) # to turn [1., 0., 0., 0.] into [0.4754, 0.1749, 0.1749, 0.1749]\n",
    "\n",
    "pred_good.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using flattening to mimick the classification case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 4]), torch.Size([64, 4]), torch.Size([64, 4]))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_flat = target.reshape([1, 4, 64])\n",
    "pred_bad_flat = pred_bad.reshape([1, 4, 64])\n",
    "pred_good_flat = pred_good.reshape([1, 4, 64])\n",
    "\n",
    "target_flat = target_flat.permute(0, 2, 1)[0]\n",
    "pred_bad_flat = pred_bad_flat.permute(0, 2, 1)[0]\n",
    "pred_good_flat = pred_good_flat.permute(0, 2, 1)[0]\n",
    "\n",
    "target_flat.shape, pred_bad_flat.shape, pred_good_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_bad = pred_bad_flat[target_flat == 1]\n",
    "pt_good = pred_good_flat[target_flat == 1]\n",
    "\n",
    "# # Which is equivalent to what before was done like this\n",
    "# pt = torch.where(target == 1, pred, 0)\n",
    "# pt = pt.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1204)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixel_fl_bad = - torch.pow(1 - pt_bad, gamma) * torch.log(pt_bad)\n",
    "fl_bad = pixel_fl_bad.mean()\n",
    "fl_bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4042)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixel_fl_good = - torch.pow(1 - pt_good, gamma) * torch.log(pt_good)\n",
    "fl_good = pixel_fl_good.mean()\n",
    "fl_good"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2985, 0.6023, 0.1072, 0.1263, 0.6341, 0.0896, 0.3313, 0.5475, 0.1245,\n",
       "        0.9195, 0.2905, 0.3415, 0.5671, 0.6836, 0.0615, 0.0411, 0.5475, 0.1193,\n",
       "        0.5202, 0.7757, 0.3471, 0.0399, 0.0388, 0.6864, 0.0644, 0.0207, 0.0962,\n",
       "        0.2078, 0.2975, 0.3145, 0.5481, 0.2197, 0.4078, 0.1834, 0.0129, 0.3133,\n",
       "        0.2000, 0.2322, 0.4959, 0.3939, 0.0679, 0.5505, 0.3391, 0.0510, 0.2323,\n",
       "        0.2390, 0.1906, 0.1828, 0.3390, 0.1070, 0.5231, 0.0141, 0.1243, 0.5086,\n",
       "        0.2491, 0.2309, 0.1206, 0.2604, 0.2431, 0.4729, 0.1216, 0.5495, 0.2166,\n",
       "        0.0394])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt = pred_bad[target == 1] # [H*W]\n",
    "\n",
    "pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt.shape # []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1204)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixel_losses = - torch.pow(1 - pt, gamma) * torch.log(pt) # [H*W]\n",
    "fl = pixel_losses.mean()\n",
    "fl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 1])\n",
      "tensor([0.3160, 0.2328, 0.2346, 0.2166])\n",
      "tensor(0.8291)\n"
     ]
    }
   ],
   "source": [
    "# Check if pixel losses make sense\n",
    "i = 5\n",
    "j = 5\n",
    "\n",
    "print(target[0, :, i, j])\n",
    "print(pred_bad[0, :, i, j])\n",
    "print(pixel_losses.reshape([8, 8])[i, j])\n",
    "\n",
    "# tensor([1, 0, 0, 0])\n",
    "# tensor([0.9195, 0.0358, 0.0117, 0.0329])\n",
    "# tensor(0.0370)\n",
    "\n",
    "# tensor([1, 0, 0, 0])\n",
    "# tensor([0.7757, 0.0320, 0.0889, 0.1034])\n",
    "# tensor(0.1206)\n",
    "\n",
    "# tensor([1, 0, 0, 0])\n",
    "# tensor([0.0411, 0.1552, 0.7253, 0.0783])\n",
    "# tensor(1.1311)\n",
    "\n",
    "# tensor([0, 0, 1, 0])\n",
    "# tensor([0.2949, 0.0735, 0.0679, 0.5637])\n",
    "# tensor(1.7795)\n",
    "\n",
    "# tensor([0, 0, 0, 1])\n",
    "# tensor([0.0232, 0.0272, 0.4409, 0.5086])\n",
    "# tensor(0.5984)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FOR A BATCH OF IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4, 8, 8]), torch.Size([2, 4, 8, 8]))"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_batch = torch.concatenate([target, target])\n",
    "pred_batch = torch.concatenate([pred_bad, pred_good])\n",
    "\n",
    "target_batch.shape, pred_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 8, 8])"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute `pt`, turning to zero the prediction for the wrong classes.\n",
    "pt_batch = torch.where(target_batch == 1, pred_batch, 0)\n",
    "\n",
    "pt_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 8])"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep only `p` of the right class.\n",
    "pt = torch.sum(pt_batch, dim=1)\n",
    "pt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 8])"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute pixel focal losses\n",
    "gamma = 2\n",
    "pixel_losses = - torch.pow(1 - pt, gamma) * torch.log(pt)\n",
    "pixel_losses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.1204, 0.4042])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.7623)"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute overall focal loss for each image in the batch\n",
    "image_losses = torch.mean(pixel_losses, dim=[1, 2])\n",
    "print(image_losses) # remember: bad prediction, good prediction\n",
    "\n",
    "# Compute batch loss\n",
    "batch_loss = torch.mean(image_losses)\n",
    "batch_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstration that when `gamma=0` we fall back to the Crossentropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FL with gamma=0\n",
    "# tensor([1.5920, 0.9468])\n",
    "# tensor(1.2694)\n",
    "\n",
    "# NOTE: can't use `torch.nn.CrossEntropyLoss()` because it applies a Softmax internally,\n",
    "# while these tensors are already activated like a Softmax would do.\n",
    "# The example in the doc page (https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
    "# implies the PyTorch function doesn't apply a Softmax internally, but it does. Proof:\n",
    "# logits = torch.randn([1, 4, 8, 8])\n",
    "# preds = torch.nn.Softmax(dim=1)(logits)\n",
    "# a = target * torch.log(preds) # [1, C, H, W]\n",
    "# pixel_losses = - torch.sum(a, dim=1) # [1, H, W]\n",
    "# ce = pixel_losses.mean()\n",
    "# ce_torch = torch.nn.CrossEntropyLoss()(input=logits, target=target.type(torch.float32))\n",
    "# ce, ce_torch\n",
    "# # (tensor(1.8743), tensor(1.8743))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5920)"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute crossentropy (just for `pred_bad`).\n",
    "# We could have used also my `WeightedCrossEntropyLoss` function, \n",
    "# (see the crossentropy notebook) but turning the internal Softmax off.\n",
    "a = target * torch.log(pred_bad) # [1, C, H, W]\n",
    "pixel_losses = - torch.sum(a, dim=1) # [1, H, W]\n",
    "ce = pixel_losses.mean()\n",
    "ce\n",
    "\n",
    "# 1.5920"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <ins> CLASSES FOR FOCAL LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class that considers just the predictions for the correct class.\n",
    "class FocalLoss1(torch.nn.Module):\n",
    "    def __init__(self, gamma, class_weights=None):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.class_weights = class_weights\n",
    "        self.activation = torch.nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, input, target):        \n",
    "        input_activated = self.activation(input)\n",
    "        # Compute `pt`, turning to zero the prediction for the wrong classes.\n",
    "        pt_batch = torch.where(target == 1, input_activated, 0) # [B, C, H, W]\n",
    "        # Keep only `p` of the right class.\n",
    "        pt = torch.sum(pt_batch, dim=1) # [B, H, W]\n",
    "        pt_log = torch.log(torch.where(pt==0, 1e-8, pt)) # safely compute log\n",
    "        pixel_losses = -torch.pow((1 - pt), self.gamma) * pt_log # [B H, W]\n",
    "        \n",
    "        if self.class_weights is not None:\n",
    "            weights = self.compute_weights(gt=target, class_weights=self.class_weights)\n",
    "            pixel_losses = pixel_losses * weights \n",
    "        \n",
    "        batch_loss = torch.mean(pixel_losses) # []\n",
    "        \n",
    "        return batch_loss\n",
    "        \n",
    "    def compute_weights(self, gt, class_weights):\n",
    "        weights = gt * class_weights.reshape(1, class_weights.shape[0], 1, 1) # [B, C, H, W]\n",
    "        weights = torch.sum(weights, dim=1) # [B, H, W]\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0843)"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(2296)\n",
    "logits = torch.randn([2, 4, 8, 8])\n",
    "fl_fn = FocalLoss1(gamma=2)\n",
    "fl_fn(input=logits, target=target_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class that considers also predictions for the wrong classes.\n",
    "class FocalLoss2(torch.nn.Module):\n",
    "    def __init__(self, gamma, class_weights=None):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.class_weights = class_weights\n",
    "        self.activation = torch.nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, input, target):        \n",
    "        input_activated = self.activation(input)\n",
    "        pt = torch.where(target==1, input_activated, 1 - input_activated)\n",
    "        pt_log = torch.log(torch.where(pt==0, 1e-8, pt)) # safely compute log\n",
    "        pixel_losses = -torch.pow((1-pt), self.gamma) * pt_log # [batch_size, n_classes, H, W]\n",
    "        summed_pixel_losses = torch.sum(pixel_losses, dim=1) # [batch_size, H, W]\n",
    "        \n",
    "        if self.class_weights is not None:\n",
    "            weights = self.compute_weights(gt=target, class_weights=self.class_weights)\n",
    "            summed_pixel_losses = summed_pixel_losses * weights \n",
    "        \n",
    "        batch_loss = torch.mean(summed_pixel_losses) # []\n",
    "        \n",
    "        return batch_loss\n",
    "        \n",
    "    def compute_weights(self, gt, class_weights):\n",
    "        weights = gt * class_weights.reshape(1, class_weights.shape[0], 1, 1) # [batch_size, n_classes, H, W]\n",
    "        weights = torch.sum(weights, dim=1) # [batch_size, H, W]\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2854)"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(2296)\n",
    "logits = torch.randn([2, 4, 8, 8])\n",
    "fl_fn = FocalLoss2(gamma=2)\n",
    "fl_fn(input=logits, target=target_batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
